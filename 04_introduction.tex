\chapter{Introduction}
\label{sec:einleitung}
Linear regression problems can be solved analytically, but nonlinear regression problems are usually analytically intractable. Soon statistics provided a new view on regression, where dimensionality did not represent an intrinsic problem anymore, but just one of computability. Principal Component Analysis was expanded to Probabilistic Principal Component Analysis \cite{Bishop_1999}, and Stochastic Processes were introduced in the 1940s with Wieners \cite{Wiener_1940} and Kolmogorovs \cite{Kolmogoroff_1941} work, while the idea for stochastic processes dates back as far as the 1880s \cite{Thiele_1880} \cite{Lauritzen_1981}. The first applicable Gaussian Processes are attributed to the field of geostatistics, where a method called \textit{kriging} \cite{Matheron_1973} \cite{Journel_1976} was introduced, which is a Gaussian Process Regression for the prediction of spatial data, spanning mostly two and three dimensional input spaces. The field of spatial statistics picked up on Gaussian processes, with overviews given in \cite{Ripley_2005}, \cite{Cressie_1993}. After the first computer experiments showed that through looking at noise free data a theory for optimising the hyper parameters can be constructed, the field started to be accepted more widespread. Williams and Rasmussen first described Gaussian Processes in a machine learning context \cite{Williams_Rasmussen_1996}, first connecting Gaussian processes to infinite nerual networks \cite{Neal_1996}. Gaussian processes also tie into regularisation theory, splines, support vector machines and relevance vector machines, other methods from the field of machine learning. \newline \newline
In the field of finance however, Gaussian processes have not been studied much. Prior to e.g. \cite{Nirwan_2019}, stochastic process models were listed in the literature to have possible applications in finance, but such tests were not widespread. Coming from modern portfolio theory \cite{Markowitz_1952}, financial problems usually were described through equations involving statistics. A problem unsolved to this day is the prediction of prices in financial contexts, e.g. stocks in a market. Prior research has shown that equally weighted portfolios outperform sample estimate portfolios \cite{Jobson_Korkie_1981}, with high data space dimensions leading to the challenging task of estimating the equally high dimensional covariance matrices. This estimation becomes even more challenging, when the number of assets is large compared to the number of observations. In those cases, the sample covariance matrix is often unstable or even singular. Factor models were then introduced, like the single index model \cite{Sharpe_1991}, or shrinkage estimators \cite{Ledoit_2004}, developed and employed in portfolio optimisation. \newline
Topical machine learning models have been used to try and optimise problems in finance. Reinforcement learning was used to try and optimally execute trades \cite{Nevmyvaka_2006}, asset prices were forecast using neural networks \cite{Gately_1995}, and even with Gaussian processes \cite{Chapados_2008}. Deep autoencoders were used to optimally allocate portfolios \cite{Heaton_2018}, and volatility models were developed using Gaussian processes too. At last, time varying covariance matrices were introduced to estimate time-varying dependencies \cite{Wilson_2011}. Bayesian methods have become more popular in this domain, since they work with random variables instead of true values, giving rise to model incorporated estimation uncertainties. This is also helpful, when estimating data with outliers. Hypotheses testing is also possible through choice of prior, where additional information can be incorporated into the model. A latent space variant of the Gaussian process models are Gaussian process latent variable models \cite{Lawrence_2005}. These models can be interpreted as a non-linear extension of standard factor models \cite{Rasmussen_06}. Parameters of the GPLVM can be treated readily interpretable, for example as market betas. Here, the sample covariance matrix is shrunk toward a more strucutred matrix given by the kernel. This matrix is supposed to maximize the likelihood function, while shrinking, and therefore systematically reduces sampling errors \cite{Nirwan_2019}. Since errors remain that are systematical, further investigation is provided in this work. All experiments were performed on subsets of S\&P500 stocks from different time periods of about three years after 2010/01/01. Several novel models were developed, a GPLVM with a latent space time dynamic, a GPLVM with infered time dependent volatility, and a Student-t Latent Variable Model, which were in turn compared to each other and the GPLVM from \cite{Nirwan_2019}. 